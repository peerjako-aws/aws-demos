{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal of this demo**\n",
    "\n",
    "We'll deploy a HPC cluster and run a simple job, using AWS ParallelCluster\n",
    "\n",
    "**What is AWS ParallelCluster**\n",
    "\n",
    "AWS ParallelCluster is a CloudFormation based environment that allows to build pre-configured HPC clusters.\n",
    "Services used:\n",
    "AWS CloudFormation, AWS Identity and Access Management (IAM), Amazon Simple Notification Service (Amazon SNS), Amazon Simple Queue Service (Amazon SQS), Amazon Elastic Compute Cloud (Amazon EC2), Amazon EC2 Auto Scaling, Amazon Elastic Block Store (Amazon EBS), Amazon Simple Storage Service (Amazon S3), Amazon DynamoDB and Amazon FSx Lustre.\n",
    "\n",
    "AWS ParallelCluster is open-source: https://github.com/aws/aws-parallelcluster\n",
    "\n",
    "Some key features in the initial release of ParallelCluster that were not in CfnCluster are:\n",
    "\n",
    "* AWS Batch integration\n",
    "* Multiple EBS volumes\n",
    "* Better scaling performance – faster, with updates AutoScaling all at once\n",
    "* Support for “bring your own AMI” Custom AMI\n",
    "* Private cluster using proxy\n",
    "\n",
    "Lets get started\n",
    "\n",
    "**Install AWS Parallel Cluster**\n",
    "\n",
    "We use python pip to install parallelcluster here: ~/.local/bin/pcluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: aws-parallelcluster in /home/ec2-user/.local/lib/python3.6/site-packages (2.3.1)\n",
      "Requirement already satisfied: future<=0.17.1,>=0.16.0 in /home/ec2-user/.local/lib/python3.6/site-packages (from aws-parallelcluster) (0.17.1)\n",
      "Requirement already satisfied: boto3>=1.9.54 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from aws-parallelcluster) (1.9.125)\n",
      "Requirement already satisfied: tabulate<=0.8.3,>=0.8.2 in /home/ec2-user/.local/lib/python3.6/site-packages (from aws-parallelcluster) (0.8.3)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.9.54->aws-parallelcluster) (0.2.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.9.54->aws-parallelcluster) (0.9.4)\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.125 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.9.54->aws-parallelcluster) (1.12.125)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.125->boto3>=1.9.54->aws-parallelcluster) (2.7.3)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.20; python_version >= \"3.4\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.125->boto3>=1.9.54->aws-parallelcluster) (1.23)\n",
      "Requirement already satisfied: docutils>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.125->boto3>=1.9.54->aws-parallelcluster) (0.14)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.125->boto3>=1.9.54->aws-parallelcluster) (1.11.0)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --user aws-parallelcluster\n",
    "!mkdir /home/ec2-user/.parallelcluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-requisites**\n",
    "\n",
    "Before starting a cluster we will create the following AWS resources using the AWS Python SDK boto3:\n",
    "* An S3 bucket that will be used for importing and exporting files to an AWS FSx Lustre file-cluster\n",
    "* A VPC with one subnet, an internet gateway and a route table with a public route. Our cluster master and worker nodes will be created in this subnet when we create our cluster\n",
    "* An EC2 key pair that we will use to ssh into the cluster master node\n",
    "\n",
    "Remember to change the bucket name to something globally unique, e.g. **your-initials-pcluster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an S3 bucket using boto3\n",
    "import boto3\n",
    "\n",
    "your_bucket_name = 'peerjako-pcluster' # CHANGE THIS!!!\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.create_bucket(Bucket=your_bucket_name,\n",
    "                         CreateBucketConfiguration={'LocationConstraint':boto3.Session().region_name})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vpc-0ffdf77a2685939c3\n"
     ]
    }
   ],
   "source": [
    "# Create a VPC\n",
    "ec2 = boto3.resource('ec2')\n",
    "ec2Client = boto3.client('ec2')\n",
    "# create VPC\n",
    "vpc = ec2.create_vpc(CidrBlock='10.0.0.0/16')\n",
    "# we can assign a name to vpc, or any resource, by using tag\n",
    "vpc.create_tags(Tags=[{\"Key\": \"Name\", \"Value\": \"pcluster_vpc\"}])\n",
    "vpc.wait_until_available()\n",
    "print(vpc.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "igw-02940828dd78790b6\n"
     ]
    }
   ],
   "source": [
    "# Create then attach internet gateway to the VPC\n",
    "ig = ec2.create_internet_gateway()\n",
    "vpc.attach_internet_gateway(InternetGatewayId=ig.id)\n",
    "print(ig.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rtb-0580c2e5abd02cfa2\n"
     ]
    }
   ],
   "source": [
    "# Create a route table and a public route\n",
    "route_table = vpc.create_route_table()\n",
    "route = route_table.create_route(\n",
    "    DestinationCidrBlock='0.0.0.0/0',\n",
    "    GatewayId=ig.id\n",
    ")\n",
    "print(route_table.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subnet-03a42c202d3745009\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ec2.RouteTableAssociation(id='rtbassoc-0434d33aa2146758a')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a subnet\n",
    "subnet = ec2.create_subnet(CidrBlock='10.0.1.0/24', VpcId=vpc.id)\n",
    "print(subnet.id)\n",
    "\n",
    "# and associate the route table with the subnet\n",
    "route_table.associate_with_subnet(SubnetId=subnet.id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '4f5beaab-c2b7-4b20-8f49-4ef569576861',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'content-type': 'text/xml;charset=UTF-8',\n",
       "   'content-length': '237',\n",
       "   'date': 'Fri, 19 Apr 2019 16:07:58 GMT',\n",
       "   'server': 'AmazonEC2'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Enabe DNS support and hostnames for VPC. This is required by parallelcluster\n",
    "ec2Client.modify_vpc_attribute( VpcId = vpc.id , EnableDnsSupport = { 'Value': True } )\n",
    "ec2Client.modify_vpc_attribute( VpcId = vpc.id , EnableDnsHostnames = { 'Value': True } )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an EC2 key pair \n",
    "keypair = ec2.create_key_pair(KeyName='aws_rsa')\n",
    "\n",
    "# and save the key material to a file: aws_rsa.pem\n",
    "with open(\"aws_rsa.pem\", \"w\") as text_file:\n",
    "    text_file.write(keypair.key_material)\n",
    "    \n",
    "!chmod 400 aws_rsa.pem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a cluster configuration**\n",
    "\n",
    "Before launching a cluster, we'll need to create a parallelcluster configuration. You can see all the configuration options here: https://aws-parallelcluster.readthedocs.io/en/develop/configuration.html\n",
    "\n",
    "Some of the configuration options we use for our demo cluster are:\n",
    "* initial_queue_size = 4  # The cluster will start with 4 worker nodes\n",
    "* scheduler = slurm  # The cluster will use the slurm scheduler. Other options are sge, torque and AWS Batch (docker containers instead of worker nodes). For a comparison between slurm, torque and sge: https://bitsanddragons.wordpress.com/2017/08/29/slurm-vs-torque-vs-sge/\n",
    "* fsx_settings = fs # Create an FSx Lustre file cluster to be used by our cluster nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[aws]\n",
      "aws_region_name = eu-west-1\n",
      "\n",
      "[cluster default]\n",
      "vpc_settings = public\n",
      "key_name = aws_rsa\n",
      "scheduler = slurm\n",
      "initial_queue_size = 4\n",
      "maintain_initial_size = true\n",
      "fsx_settings = fs\n",
      "\n",
      "[vpc public]\n",
      "master_subnet_id = subnet-03a42c202d3745009\n",
      "vpc_id = vpc-0ffdf77a2685939c3\n",
      "\n",
      "[global]\n",
      "update_check = true\n",
      "sanity_check = true\n",
      "cluster_template = default\n",
      "\n",
      "[fsx fs]\n",
      "shared_dir = /fsx\n",
      "storage_capacity = 3600\n",
      "import_path = s3://peerjako-pcluster\n",
      "imported_file_chunk_size = 1024\n",
      "export_path = s3://peerjako-pcluster/export\n",
      "weekly_maintenance_start_time = 1:00:00\n",
      "\n",
      "[aliases]\n",
      "ssh = ssh {CFN_USER}@{MASTER_IP} {ARGS}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define cluster config\n",
    "pcluster_config = (\n",
    "    '[aws]\\n'\n",
    "    'aws_region_name = ' + boto3.Session().region_name + '\\n'\n",
    "    '\\n'\n",
    "    '[cluster default]\\n'\n",
    "    'vpc_settings = public\\n'\n",
    "    'key_name = ' + keypair.key_name + '\\n'\n",
    "    'scheduler = slurm\\n'\n",
    "    'initial_queue_size = 4\\n'\n",
    "    'maintain_initial_size = true\\n'\n",
    "    'fsx_settings = fs\\n'\n",
    "    '\\n'\n",
    "    '[vpc public]\\n'\n",
    "    'master_subnet_id = ' + subnet.id + '\\n'\n",
    "    'vpc_id = ' + vpc.id + '\\n'\n",
    "    '\\n'\n",
    "    '[global]\\n'\n",
    "    'update_check = true\\n'\n",
    "    'sanity_check = true\\n'\n",
    "    'cluster_template = default\\n'\n",
    "    '\\n'\n",
    "    '[fsx fs]\\n'\n",
    "    'shared_dir = /fsx\\n'\n",
    "    'storage_capacity = 3600\\n'\n",
    "    'import_path = s3://' + bucket.name + '\\n'\n",
    "    'imported_file_chunk_size = 1024\\n'\n",
    "    'export_path = s3://' + bucket.name + '/export\\n'\n",
    "    'weekly_maintenance_start_time = 1:00:00\\n'\n",
    "    '\\n'\n",
    "    '[aliases]\\n'\n",
    "    'ssh = ssh {CFN_USER}@{MASTER_IP} {ARGS}\\n')\n",
    "\n",
    "with open(\"/home/ec2-user/.parallelcluster/config\", \"w\") as text_file:\n",
    "    text_file.write(pcluster_config)\n",
    "    \n",
    "print(pcluster_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Copying demo files into the bucket**\n",
    "\n",
    "For our demo we will need a couple of files (more info on those later). We copy these files into our bucket so that they are available to the cluster through the Luster file-cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.Bucket(bucket.name).upload_file('pi-mpi.c','pi-mpi.c')\n",
    "s3.Bucket(bucket.name).upload_file('batch.sh','batch.sh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the AWS parallelcluster**\n",
    "\n",
    "We are now ready to create the cluster using the \"pcluster create\" command. \n",
    "The cluster can take 10-20 minutes to create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning cluster creation for cluster: hello-cluster1\n",
      "Creating stack named: parallelcluster-hello-cluster1\n",
      "Status: SQSPolicy - CREATE_IN_PROGRESS                                          "
     ]
    }
   ],
   "source": [
    "# Create our first cluster and call it hello-cluster1\n",
    "!~/.local/bin/pcluster create hello-cluster1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check out the cluster resources**\n",
    "\n",
    "pcluster uses AWS CloudFormation to create AWS resources (infrastructure as code):\n",
    "\n",
    "https://eu-west-1.console.aws.amazon.com/cloudformation/home\n",
    "\n",
    "The FSx Lustre file cluster can be seen here:\n",
    "\n",
    "https://eu-west-1.console.aws.amazon.com/fsx/home#file-systems\n",
    "\n",
    "Once the file cluster is created you can see that pcluster creates master and worker EC2 instances:\n",
    "\n",
    "https://eu-west-1.console.aws.amazon.com/ec2/v2/home#Instances:sort=instanceId"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SSH into the cluster and test**\n",
    "\n",
    "Open a terminal and ssh into the cluster:\n",
    "\n",
    "`~/.local/bin/pcluster ssh hello-cluster1 -i aws_rsa.pem`\n",
    "\n",
    "Let's check whether we have compute nodes available:\n",
    "\n",
    "`sinfo`\n",
    "\n",
    "Let's run a simple job to check that everything is working the right way:\n",
    "\n",
    "`srun -n 1 /bin/hostname`\n",
    "\n",
    "Now let's run it on four nodes instead of a single one:\n",
    "\n",
    "`srun -n 4 /bin/hostname`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate PI**\n",
    "\n",
    "Ok, so we now have a cluster and we have checked that we were able to run a job on several node.\n",
    "\n",
    "Now let's do something slightly more complex and execute an mpi job that calculates the Pi number. \n",
    "\n",
    "Change directory into the Lustre partition and check the content of the file **pi-mpi.c**:\n",
    "\n",
    "`cd /fsx && cat pi-mpi.c`\n",
    "\n",
    "Gcc and openmpi have been pre-installed by AWS ParallelCluster, so we just need to setup the environment:\n",
    "\n",
    "`module load mpi && which mpicc`\n",
    "\n",
    "Ok, now we got everything we need to compile our small mpi application:\n",
    "\n",
    "`mpicc -v -lm -o pi-mpi pi-mpi.c`\n",
    "\n",
    "So we should now have a hello-mpi binary ready to be launched. We have a simple batch.sh bash file that we will use to run a batch job:\n",
    "\n",
    "`cat batch.sh`\n",
    "\n",
    "We can now submit it to the queue: \n",
    "\n",
    "`sbatch -n 4 ./batch.sh`\n",
    "\n",
    "and check the status of the queue:\n",
    "\n",
    "`squeue`\n",
    "\n",
    "The job might be pending, waiting for resources to be launched by AWS ParallelCluster.\n",
    "Wait for the resource to be launch, and once the job is finished, you'll find a slurm-<jobID>.out file in your directory, containing the output of the job:\n",
    "    \n",
    "`tail -f slurm-*.out`\n",
    "    \n",
    "Once you have the PI result hit control-c to exit the file tail.\n",
    "    \n",
    "**Here we are, you have ran your first MPI parallel job on AWS, congratulations !**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Play with elasticity**\n",
    "\n",
    "Submit several jobs in the queue (sbatch), look at the available resources (sinfo), look at the queue (squeue) and see how the situation evolves.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recover the job files**\n",
    "\n",
    "Both the master node and worker nodes have the FSx file partition mounted at /fsx\n",
    "\n",
    "Using the lfs client you can archive the files of your pcluster (both master and workers) to your S3 buckets export path.\n",
    "\n",
    "We have our slurm output file(s) on the master node so lets archive the to the S3 bucket:\n",
    "\n",
    "`sudo lfs hsm_archive /fsx/*.out`\n",
    "\n",
    "Read more about how to use the lustre client with S3 here: \n",
    "\n",
    "https://docs.aws.amazon.com/fsx/latest/LustreGuide/fsx-data-repositories.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**List job output files from the Lustre files that were exported into the S3 bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for object_summary in bucket.objects.filter(Prefix=\"export/\"):\n",
    "    print(object_summary.key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tear down the cluster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the cluster\n",
    "!~/.local/bin/pcluster delete hello-cluster1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Delete the other AWS resources (VPC, EC2 key pair and S3 bucket)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the VPC\n",
    "ec2Client.delete_subnet(SubnetId = subnet.id)\n",
    "ec2Client.delete_route_table(RouteTableId = route_table.id)\n",
    "ec2Client.detach_internet_gateway(InternetGatewayId = ig.id, VpcId = vpc.id)\n",
    "ec2Client.delete_internet_gateway(InternetGatewayId = ig.id)\n",
    "ec2Client.delete_vpc(VpcId = vpc.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the EC2 key pair and the pem file\n",
    "ec2Client.delete_key_pair(KeyName = keypair.key_name)\n",
    "!rm -rf aws_rsa.pem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the bucket - Warning! This will also delete all the files.\n",
    "bucket.objects.all().delete()\n",
    "bucket.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Documentation**\n",
    "\n",
    "https://aws-parallelcluster.readthedocs.io/en/latest/index.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
